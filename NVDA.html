<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>NVDA</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>


</head>

<body>

<h1 id="toc_0">NVDA</h1>

<p>GCon = 1 to 5 scale of ‘Gregs conviction’.<br>
Most buzzwords are in the &lsquo;Definitions&rsquo; section.</p>

<h2 id="toc_1">My understanding</h2>

<p>Nvidia produces GPU (graphical processing unit) chips. GPUs are developed to be massively parallel floating point processors, which excel at rendering graphics for games.</p>

<p>Think of CPUs which are single-core, dual-core, quad-core, sometimes 8-16 cores. These are the parallel processing brains, so a quad-core CPU can process four concurrent tasks. CPUs are optimised for sequential serial processing.</p>

<p>GPUs on the other hand have thousands of less powerful cores, which means they can run thousands of simpler tasks concurrently. For example, the Geforce GTX 1080Ti has 3584 cores.</p>

<p>There are a bunch of technologies that benefit from massive parallel processing. Graphics, cryptocurrency mining, AI, etc. They all do (more or less) the same thing with slightly different data. These are the big tides lifting Nvidias boat.</p>

<p>The major point is that currently GPUs are the &lsquo;solution-du-jour&rsquo; for ubiquitous parallel processing. There are other technologies that are better for some situations, but in general, GPUs are the best at solving the general problem.</p>

<h3 id="toc_2">Stock advisor</h3>

<p>Their latest recommendation (Jan 2017) was at a stock price of $101. A large part of the premise was the growth of automated vehicle revenue. <a href="https://www.fool.com/premium/stock-advisor/coverage/18/coverage/updates/2017/01/20/davids-pick-nvidia.aspx">Latest recommmedation</a></p>

<p>They mention <q>In fact, analysts are predicting 5.5 million semi- and fully autonomous cars shipped in 2020</q> which seems&hellip; optimistic to me?</p>

<h3 id="toc_3">Nvidia products</h3>

<ul>
<li>DGX-2 - AI System (<q>worlds most powerful</q>). Computer + AI frameworks</li>
<li>Drive - System on a chip for Autonomous vehicles</li>
<li>GeForce - Gaming GPUs - Note, these are incorporated into graphics cards.</li>
<li>Virtual GPU - Share a remote GPU with laptop/desktop clients, i.e., the GPU is running in the cloud, the processing takes place there and the data is sent back to the client. Not quite sure who owns/runs the hardware.</li>
<li>Jetson - “Credit card sized supercomputers” - Embedded AI computing. So if you want onboard AI (e.g.: train in AWS, transfer for inference on mobile robots/machines) [$580USD]</li>
<li>Quadro - Pro-version GeForce [more precise, ‘better’] cards for use in high end visualisation (e.g.: Weta, Oil and Gas etc). I believe Nvidia manufacturer these cards, rather than just the chips. They’re heaps more expensive than the equivalent GeForce.</li>
<li>Shield TV - 4K HDR STREAMING MEDIA PLAYER - GD: Does anyone use this? Seems pretty random. Seems to have access to GeForce Now which suggests that games are rendered in Nvidia cloud and streamed to the Shield? - yep, but still very much in beta - not sure how this would deal with latency.</li>
<li>Tesla - Tesla is the AI focused card which implements the Volta-spec (as opposed to Pascal-spec etc). Tesla is designed to be deployed into data centres. This is the big growth area for Nvidia. </li>
<li>Tegra - Their mobile &lsquo;System on a chip&rsquo;. Used in the Nintendo switch, and I think the MSFT Surface 2, as well as Drive. From the Q4 2018 CFO summary, <q>Tegra Processor business revenue includes SOC modules for the Nintendo Switch gaming console and development services. Also included is automotive revenue of $132 million&hellip;incorporating infotainment modules, production DRIVE PX platforms, and development agreements for self-driving cars.</q></li>
</ul>

<h1 id="toc_4">Segments</h1>

<p>In their 2018 investor presentation, Nvidia break out their big segments:</p>

<ol>
<li>Gaming [GeForce]</li>
<li>Pro-visualisation [Quadro]</li>
<li>Datacenter [Tesla]</li>
<li>Autonomous vehicles</li>
</ol>

<h2 id="toc_5">Datacenter</h2>

<p>Revenue growth: FY2016-17 = <u>145%</u>
Revenue growth: FY2017-18 = <u>133%</u></p>

<p>Nvida estimate a TAM of $50b, while AMDs estimate is $21b.  </p>

<p>Three major areas of datacenter growth:</p>

<ol>
<li>High Performance Computing <strong>TAM: $10b</strong></li>
<li>Hyperscale and consumer internet (recommendation engine, credit scoring, fraud detection, ad insertion, ai assistant) <strong>TAM: $20b</strong></li>
<li>Cloud computing and industries (startups, healthcare, transport, manufacturing, public sector, oil and gas, + 50% ‘future industries’) <strong>TAM: $10b</strong></li>
</ol>

<p>I believe this is where most of Nvidias immediate potential lies. The new Titan cards with included Tensor cores indicate the flexibility of Nvidias chip making capability, which will allow them to compete sucessfully in this area.</p>

<p>This (I believe) will be the immediate number to watch. The other segments I believe will either hold steady or slow growth in the short term.</p>

<p><strong>GCon:4/5</strong></p>

<h2 id="toc_6">Automotive</h2>

<p>Revenue growth: FY2017-18 = <u>15%</u></p>

<blockquote>
<p>AV is a $60b opportunity by 2035.</p>
</blockquote>

<p>Using the same architecture [Xavier] for L2 [human backup] -&gt; L5[robo-taxi]. Xavier replaces four separate computers.
Note that the edge processing (Xavier) is for data gathering, pre-processing and inference. The training takes place on other systems in big datacenters. Nvidia could have a pretty compelling case to be the &lsquo;supercomputer-in-the-car&rsquo; while training takes place on other systems.</p>

<p>Drive Sim and Constellation: Simulate real world driving conditions to train AI systems. 370+ partners developing on Drive Sim.</p>

<p>Definitely a case of providing shovels to gold prospectors. Nvidia is developing the tools to let AV contenders play in the space without developing their own chips. Even as a proof-of-concept tool, I imagine the commercial case is pretty compelling. It&rsquo;s only large very well resourced players (eg: Google) who use their own chips/software.</p>

<div><pre><code class="language-none">Q: Google/Waymo seems to be the runaway leader in AutomotiveAutonomous vehicles. Is the a winner-take-all situation?/
A: Unclear to me. I can imagine a future where Waymo starts selling their tech to automotive companies, which would be a disaster for Nvidia’s (and others) programs. However, Waymo has no real partners in the automotive industry to date. Although conceptually it makes sense, those companies may (rightly!) be worried about ceding control of the experience to Google/Waymo, essentially giving up a huge chunk of independence to a brutal competitor.</code></pre></div>

<p>This market should be watched carefully. I&rsquo;m guessing it will gradually increase in revenue for Nvidia but I&rsquo;m uncertain whether they will end up powering the majority of autonomous vehicles (however, if not them, then who?). However, they&rsquo;re definitely one of the primary horses in the race, the other being Google. </p>

<p><strong>Red flag</strong> - If Google start making significant deals with automotive companies.</p>

<p><strong>GCon: 3/5</strong></p>

<h2 id="toc_7">Gaming and Crypto</h2>

<p>Revenue growth: FY2017-18 = <u>21%</u></p>

<p>World-wide gaming revenue around 100b (growth of 3x since 2007)
Increasing cinematics -&gt; increasing demand for new chips.</p>

<p>ESports
PC/Console/Mobile
VR is another potential driver. Unknown in contribution and timeframe.</p>

<h3 id="toc_8">Competition</h3>

<p>Seems that AMD is the only real graphics card competition in the gaming space with their Radeon cards. Sounds like NVDA has the edge, but AMD seems to be fairly equivalent. Of course, other chip manufacturers like Intel produce GPU capability but in terms of gaming, only AMD are true competition. Both the XBox 1 and PS4 are powered by AMD GPUs, but margins are reportedly low.</p>

<p>I&rsquo;m not sure that gaming is a big growth driver for Nvidia. They&rsquo;re pretty positive about it in the investor presentation, but other resources suggest that the market is growing, but not super-rapidly. </p>

<p>(6% CAGR suggested here)[<a href="https://newzoo.com/insights/articles/the-global-games-market-will-reach-108-9-billion-in-2017-with-mobile-taking-42/">https://newzoo.com/insights/articles/the-global-games-market-will-reach-108-9-billion-in-2017-with-mobile-taking-42/</a>]</p>

<p>My concern is that Gaming GPUs will hit a &hellip; good enough level, and the market will be a market of diminishing returns, less people willing to spend for GPUs, pushing prices of GPUs down.</p>

<h3 id="toc_9">Crypto</h3>

<div><pre><code class="language-none">Q: How much of the demand is because of cryptocurrency mining?
A: [Chinese Crypto Mining Company Poses a Threat to AMD and Nvidia - Bloomberg](https://www.bloomberg.com/news/articles/2018-04-04/chinese-crypto-mining-hardware-putting-amd-nvidia-under-threat) - “5% of current revenue”
A: [Cryptocurrency Mining Sales Cool in Q3, Says Nvidia - CoinDesk](https://www.coindesk.com/cryptocurrency-mining-chip-sales-cool-q3-says-nvidia/) 70m down from $150m in Q2</code></pre></div>

<p>Not clear impact of Crypto. AMD CEO Lisa Su - (AMD: Cryptocurrency Mining Isn’t ‘A Long-Term Growth Driver’ - CoinDesk)[<a href="https://www.coindesk.com/amd-cryptocurrency-mining-isnt-a-long-term-growth-driver/">https://www.coindesk.com/amd-cryptocurrency-mining-isnt-a-long-term-growth-driver/</a>]</p>

<p>But Huang bullish on Crypto.</p>

<h4 id="toc_10">ASICS</h4>

<blockquote>
<p>“GPUs are used to mine cryptocurrencies like Ethereum and Litecoin that use the <q>scrypt</q> hashing algorithm. Bitcoin, by contrast, is chiefly mined today using dedicated hardware called ASICs.”</p>
</blockquote>

<p>ASICs as mentioned previously are custom chips for a single function. These chips are by definition more efficient than more general purpose chips like GPUs. Bitmain is a Chinese producer of ASICs for bitcoin mining (and other currencies based on SHA256 hash algorithms).</p>

<p><a href="http://bitmain.com">Asic Bitcoin Mining Hardware From Bitmain</a></p>

<blockquote>
<p>GPUs are used to mine cryptocurrencies like ethereum and litecoin that use the <q>scrypt</q> hashing algorithm. Bitcoin, by contrast, is chiefly mined today using dedicated hardware called ASICs.
GD: I assume &lsquo;scrypt&rsquo; algorithms can also be hashed using ASICs. </p>
</blockquote>

<p>Update: Bitmain are now producing ASICs for other currencies (including Ethereum). The problem with ASICs is that they&rsquo;re expensive, which means only relatively wealthy miners can utilise them. Because they&rsquo;re so powerful in terms of hashing, other methods of hashing become less useful, which means that the crypto-currency becomes centralised in the hands of the ASIC owners.</p>

<p>Not a great outcome when the point of crypto-currencies is decentralisation.</p>

<p><a href="https://www.coindesk.com/anti-asic-revolt-just-far-will-cryptos-hardware-war-go/">The Anti-ASIC Revolt</a></p>

<blockquote>
<p>GD: I&rsquo;m not convinced that crypto currencies will be a big growth driver over the longer term for NVidia. The future of crypto is pretty murky to me, and not clear how they will be &lsquo;mined&rsquo; in the future.</p>
</blockquote>

<p><strong>GCon 3/5</strong></p>

<h2 id="toc_11">AI</h2>

<p>AI (I think) is the overarching theme driving Nvidia&rsquo;s success although not specifically a &lsquo;segment&rsquo; identified by Nvidia. AI is in a peak now, due to the stunning sucesses that have been demonstrated over the last few years, and the continued improvement in those algorithms. There doesn&rsquo;t appear to be any significant roadblocks standing in the way of more and more artificial intelligence in the world.</p>

<p>More and more data, more and more sensors, more and more processing required, more and more intelligence. Cars (as a very simple example), will be more and more autonomous which means some significant processing is required to prepare data for inference (Nvidias drive program). Nvidia metropolis offers the same attractiveness for &lsquo;smart cities&rsquo; (no idea what the market is for that, but definitely in the same trend space).</p>

<p>Although the Internet of Things has been a buzzword for a while, the reality is there are more and more devices that are connected, and sending information to be recorded, and hopefully processed. All this results in a massive demand for efficient computation.</p>

<h3 id="toc_12">OpenCL versus CUDA</h3>

<p>OpenCL and CUDA do the same thing. They interfance higher level languages to GPU-specific instructions. CUDA is Nvidias interface, while OpenCL is the open-source &lsquo;standard&rsquo; supported by AMD amongst others.</p>

<p>The high-level frameworks that researchers use, TensorFlow, Torch, Theanos etc., do not really care whether its CUDA or OpenCL under the covers. However, most of the popular deep learning frameworks do not currently support OpenCL 
<a href="https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software">Deep learning software comparison</a>. In the big ones, it appears to be under development  [Theano looks like it works, Torch has 3rd party implementations]. </p>

<p>OpenCL is definitely a contender to CUDA. The open-source nature may also be attractive IFF performance can approach CUDA.</p>

<p>Nvidia has allocated significant resources to making the CUDA framework (the framework that talks to Nvidias GPUs for parallel processing) very accessible to AI researchers. It&rsquo;s also very performant.</p>

<p>Does it matter? Nvidia chips work fine with OpenCL. Other chips (eg: AMD) don&rsquo;t work with CUDA. Ultimately, the speed of processing will matter. Nvidia/CUDA is definitely the default at the moment, but I&rsquo;m not sure there is a long-term moat for CUDA. Coders will deal with either, and supporting both CUDA and OpenCL will probably not be super difficult, since they do the same thing. </p>

<h3 id="toc_13">Competition</h3>

<ol>
<li>Google (TPU)</li>
<li>FPGA (eg: Intel and Microsoft)</li>
<li>China (and Russia?) </li>
</ol>

<h4 id="toc_14">Google Tensor Processing Unit (TPU)</h4>

<p>The TPU is a custom designed chip from Google that is intended to make Deep Learning (very large neural networks) more efficient than standard GPUs. Google has recently revealed the 2nd generation chip (which suggests than there will be more generations to come). The original TPU was best for inference (ie, using a trained model), and required GPUs to train the model. TPU2 allows efficient training as well. </p>

<p>Nvidia is including Tensor Cores in its new Titan chips, so the performance comparison is unclear. However, it is highly likely that an ASIC (TPU) will perform better on those specific functions.</p>

<blockquote>
<p>GD: I think it unlikely however that the scope of architecture thats useful in AI is actually known at this point. Deep learning is one facet of AI, and other upcoming fields may benefit from the more general capabilities of GPUs.</p>
</blockquote>

<h4 id="toc_15">FPGAs</h4>

<p>Field programmable gate arrays are chips that are configurable at a hardware level after production. An ASIC (eg: TPU) on the other hand is fixed after production.</p>

<blockquote>
<p>&hellip;beginning in early 2017, GPUs have begun to face some competition from Field Programmable Gate Arrays (FPGAs). FPGAs can also accelerate Machine Learning and Artificial Intelligence workloads. [<a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">https://en.wikipedia.org/wiki/Graphics_processing_unit</a>]</p>

<p>what if new AI computing models do not conform to the orderly mold of array-based, data-parallel algorithms that GPUs are so good at processing?
<a href="http://eecatalog.com/fpga/2017/10/24/artificial-intelligence-where-fpgas-surpass-gpus/">http://eecatalog.com/fpga/2017/10/24/artificial-intelligence-where-fpgas-surpass-gpus/</a></p>
</blockquote>

<h4 id="toc_16">China (and Russia?)</h4>

<p>Its a bit vague, but the Chinese government has been very clear about its emphasis on AI. It seems highly unlikely that they will be content to be dependent on Western technology in the AI race. Russia has made similar noises.</p>

<p><a href="https://www.technologyreview.com/s/609954/china-wants-to-make-the-chips-that-will-add-ai-to-any-gadget">https://www.technologyreview.com/s/609954/china-wants-to-make-the-chips-that-will-add-ai-to-any-gadget</a></p>

<blockquote>
<p>The chip is just one example of an important trend sweeping China’s tech sector. The country’s semiconductor industry sees a unique opportunity to establish itself amid the current wave of enthusiasm for hardware optimized for AI.</p>
</blockquote>

<p>I believe for the foreseeable future, Nvidia has a strong place in any AI pipeline. Anyone who doesnt have access to custom chips will be dealing with either CUDA/OpenCL through the high-level AI frameworks.</p>

<p>**GCon 4/5 **</p>

<h2 id="toc_17">Numbers</h2>

<p>Revenue growth: FY2017-18 = <u>41%</u></p>

<table>
<thead>
<tr>
<th>Q</th>
<th>Q1 17</th>
<th>Q2 17</th>
<th>Q3 17</th>
<th>Q4 17</th>
<th>Q1 18</th>
<th>Q2 18</th>
<th>Q3 18</th>
<th>Q4 18</th>
</tr>
</thead>

<tbody>
<tr>
<td>Gaming</td>
<td>687</td>
<td>781</td>
<td>1244</td>
<td>1348</td>
<td>1027</td>
<td>1186</td>
<td>1561</td>
<td>1739</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>14%</td>
<td>59%</td>
<td>8%</td>
<td>-24%</td>
<td>15%</td>
<td>32%</td>
<td>11%</td>
</tr>
<tr>
<td>Prof vis</td>
<td>189</td>
<td>214</td>
<td>207</td>
<td>225</td>
<td>205</td>
<td>235</td>
<td>239</td>
<td>254</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>13%</td>
<td>-3%</td>
<td>9%</td>
<td>-9%</td>
<td>15%</td>
<td>2%</td>
<td>6%</td>
</tr>
<tr>
<td>Datacenter</td>
<td>143</td>
<td>151</td>
<td>240</td>
<td>296</td>
<td>409</td>
<td>416</td>
<td>501</td>
<td>606</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>6%</td>
<td>59%</td>
<td>23%</td>
<td>38%</td>
<td>2%</td>
<td>20%</td>
<td>21%</td>
</tr>
<tr>
<td>Automotive</td>
<td>113</td>
<td>119</td>
<td>127</td>
<td>128</td>
<td>140</td>
<td>142</td>
<td>144</td>
<td>132</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>5%</td>
<td>7%</td>
<td>1%</td>
<td>9%</td>
<td>1%</td>
<td>1%</td>
<td>-8%</td>
</tr>
<tr>
<td>OEM and IP</td>
<td>173</td>
<td>163</td>
<td>186</td>
<td>176</td>
<td>156</td>
<td>251</td>
<td>191</td>
<td>180</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-6%</td>
<td>14%</td>
<td>-5%</td>
<td>-11%</td>
<td>61%</td>
<td>-24%</td>
<td>-6%</td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>Segment</th>
<th>FY2017.</th>
<th>FY2018</th>
<th>% change</th>
</tr>
</thead>

<tbody>
<tr>
<td>Gaming</td>
<td>4060</td>
<td>5513</td>
<td><strong>36%</strong></td>
</tr>
<tr>
<td>Prof vis</td>
<td>835</td>
<td>933</td>
<td><strong>12%</strong></td>
</tr>
<tr>
<td>Datacenter</td>
<td>830</td>
<td>1932</td>
<td><strong>133%</strong></td>
</tr>
<tr>
<td>Automotive</td>
<td>487</td>
<td>558</td>
<td><strong>15%</strong></td>
</tr>
<tr>
<td>OEM and IP</td>
<td>698</td>
<td>778</td>
<td><strong>11%</strong></td>
</tr>
<tr>
<td><strong>Totals</strong></td>
<td><strong>6910</strong></td>
<td><strong>9716</strong></td>
<td><strong>41%</strong></td>
</tr>
</tbody>
</table>

<h3 id="toc_18">DCF</h3>

<p>Using a compounded revenue growth rate of 25% over the next 5 years and an EBIT margin of 37% (as per current year) and a WACC of 9.6% gives an estimated share value of <u>$202</u>.</p>

<p>Over the ten year period, revenue will grow to around $50b, around about the current revenues of Disney (2017 - $55b), Cisco ($49b), and Intel ($59b).</p>

<p>Any decrease in revenue growth or margins (all things equal) will decrease this share price.</p>

<h4 id="toc_19">DCF examples</h4>

<table>
<thead>
<tr>
<th>&mdash;&mdash;&mdash;</th>
<th>5 year compounded revenue growth rate</th>
<th>20%</th>
<th>25%</th>
<th>30%</th>
<th>40%</th>
</tr>
</thead>

<tbody>
<tr>
<td>Operating margin</td>
<td>Implied 10 year revenue.</td>
<td>$38b</td>
<td>$57b</td>
<td>$75b</td>
<td>$129b</td>
</tr>
<tr>
<td>20%</td>
<td></td>
<td>$93</td>
<td>$118</td>
<td>$149</td>
<td>$237</td>
</tr>
<tr>
<td>30%</td>
<td></td>
<td>$140</td>
<td>$179</td>
<td>$228</td>
<td>$367</td>
</tr>
<tr>
<td>40%</td>
<td></td>
<td>$194</td>
<td>$248</td>
<td>$318</td>
<td>$516</td>
</tr>
</tbody>
</table>

<h4 id="toc_20">Comparisons</h4>

<p>Revenues (2017) for comparison include Amazon ($135b), Microsoft ($85b), Alphabet ($90b), Cisco ($49b), Oracle ($37b).</p>

<table>
<thead>
<tr>
<th>Company</th>
<th>Rev (TTM)</th>
<th>Rev Growth%</th>
<th>OpMargin%</th>
<th>NP %</th>
</tr>
</thead>

<tbody>
<tr>
<td>Amazon</td>
<td>$93</td>
<td>$118</td>
<td>$149</td>
<td>$237</td>
</tr>
<tr>
<td>Microsoft</td>
<td>$140.</td>
<td>$179</td>
<td>$228</td>
<td>$367</td>
</tr>
<tr>
<td>Alphabet</td>
<td>$111b</td>
<td>23%</td>
<td>24%</td>
<td>20%</td>
</tr>
</tbody>
</table>

<h4 id="toc_21">Other numbers</h4>

<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>

<tbody>
<tr>
<td>PEG</td>
<td>.6</td>
</tr>
<tr>
<td>P/E (MRY)</td>
<td>49</td>
</tr>
<tr>
<td>Gross Margin (TTM).</td>
<td>60%</td>
</tr>
<tr>
<td>Operating Margin (TTM)</td>
<td>33%</td>
</tr>
<tr>
<td>Net profit Margin (TTM)</td>
<td>30%</td>
</tr>
</tbody>
</table>

<h2 id="toc_22">Definitions</h2>

<ul>
<li>ASIC - Application specific integrated circuit - specific chips used to mine bitcoin.</li>
<li>CUDA - CUDA is the leading proprietary (Nvidia) GPGPU framework [See OpenCL]</li>
<li>CUDA core - processes floating point operations (multiply-accumulate) <a href="https://devblogs.nvidia.com/programming-tensor-cores-cuda-9/">Programming Tensor Cores in CUDA 9 | NVIDIA Developer Blog</a></li>
<li>Chip Yield - The number of chips successfully created from a silicon wafer. Manufacturing defects cause chip failures. The larger the chip [die size], the more failures can be expected, leading to higher prices for larger chips.</li>
<li>Die size - the size of the chunk on a silicon wafer that corresponds to one chip.</li>
<li>Drive - Nvidias autonomous vehicle programe. Creating an ecosystem around cars. System on a card for Autonomous vehicles.</li>
<li>FPGA - Field-programmable gate array. Chips that can be reconfigured &lsquo;in the field&rsquo; for particular purposes. Both MSFT and INTC are betting on this approach for AI, so direct competition to general GPUs and specific chips (eg: TensorFlow)</li>
<li>GPGPU - GPGPU allows information to be transferred in both directions, from CPU to GPU and GPU to CPU. Such bidirectional processing can hugely improve efficiency in a wide variety of tasks related to images and video</li>
<li>OpenCL - OpenCL is currently the leading open source GPGPU framework [see CUDA]</li>
<li>Pegasus - A codename for a processor/card in the NVidia Drive project. The successor to the Drive PX 2.</li>
<li>SM - streaming multiprocessor - more SMs allows more parallel processors. SMs consist of multiple stream processors, which deal with one thread at a time.</li>
</ul>

<h2 id="toc_23">- Tensor core - A processor that deals with floating point operations on 4x4 matrices.</h2>

<h3 id="toc_24">Architectures</h3>

<ul>
<li>Maxwell - GPU architecture specification [Gen latest-2]</li>
<li>Pascal - GPU architecture specification [Gen latest- 1]</li>
<li>Volta - GPU architecture specification [Gen latest]</li>
<li>Tesla - An implementation of Volta</li>
<li>Titan V - A consumer level implementation of Volta.</li>
<li>Titan Xp - A consumer level implementation of Pascal [predecessor of Titan V]</li>
<li>Xavier - Implementation of the Volta architecture for automobiles. <q>Xavier is the most complex system on a chip ever created</q>.</li>
</ul>

<h3 id="toc_25">Rambling notes</h3>

<p>“It should be noted that Nvidia cards actually support OpenCL as well as CUDA, they just aren’t quite as efficient as AMD GPUs when it comes to OpenCL computation. This is changing though as the recently released Nvidia GTX 980 is a very capable OpenCL card as well as a CUDA monster. We can only see Nvidia’s OpenCL performance getting better and better in the future, and this is definitely something worth considering.”</p>

<p>The only situation in which we would recommend an AMD GPU to professionals is when they are exclusively using apps that support OpenCL and have no CUDA option.</p>

<blockquote>
<p>GD: If AMD put Nvidia-level support behind OpenCL…? CUDA would no longer be the obvious choice. The longer AMD leave this, the more developers will be experienced with CUDA. Apps (e.g.: Adobe suite etc) talk to CUDA/OpenCL so developer experience counts.</p>
</blockquote>

<p>There is also a crossover with gaming where the ubiquity of Nvidia cards means there is more incentive to go down a CUDA-route than the OpenCL route.</p>




</body>

</html>
